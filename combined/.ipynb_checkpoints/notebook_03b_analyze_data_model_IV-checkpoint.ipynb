{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_size_rec(data_src, data_labels, rejection_region, desired_power):\n",
    "    \n",
    "    import random\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    data_src.columns = list(data_labels)\n",
    "    \n",
    "    absolute_mde = data_src[data_src['Treated'] == 1]['Order_Amt'].mean() - \\\n",
    "                   data_src[data_src['Treated'] == 0]['Order_Amt'].mean()\n",
    "\n",
    "    \n",
    "    print(\"The absolute MDE was estimated as {}.\".format(absolute_mde))\n",
    "    \n",
    "    df = data_src[data_src['Treated'] == 0]\n",
    "    assignment = []\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        assignment.append(random.randint(0,1)) \n",
    "        i += 1\n",
    "    df['Partition'] = assignment\n",
    "    power_analysis_df = df[df['Partition'] == 0]\n",
    "    analysis_df = df[df['Partition'] == 1]\n",
    "        \n",
    "    del df\n",
    "    \n",
    "    pa_retailer_means = pd.DataFrame(power_analysis_df.groupby(['Retailer_ID'])['Order_Amt'].mean())\n",
    "    pa_retailer_means.reset_index(inplace=True)\n",
    "    pa_retailer_means.columns = ['Retailer_ID', 'Mean_Retailer_Order_Amt']\n",
    "    ###############################################################################\n",
    "    pa_dow_means = pd.DataFrame(power_analysis_df.groupby(['Dow_Rand'])['Order_Amt'].mean())\n",
    "    pa_dow_means.reset_index(inplace=True)\n",
    "    pa_dow_means.columns = ['Dow_Rand', 'Mean_DOW_Order_Amt']\n",
    "    ###############################################################################\n",
    "    analysis_df = pd.merge(analysis_df, pa_retailer_means, on='Retailer_ID', how='left')\n",
    "    analysis_df = pd.merge(analysis_df, pa_dow_means, on='Dow_Rand', how='left')\n",
    "    ###############################################################################\n",
    "    analysis_df = analysis_df[['Order_Amt', 'Customer_ID', 'Mean_Order_Amt', \n",
    "                               'Mean_Retailer_Order_Amt','Mean_DOW_Order_Amt']]\n",
    "    ###############################################################################\n",
    "    analysis_df = analysis_df.dropna(how = 'any')\n",
    "    \n",
    "#     X = analysis_df[['Mean_Order_Amt', 'Mean_Retailer_Order_Amt','Mean_DOW_Order_Amt']]\n",
    "#     X = analysis_df[['Mean_Retailer_Order_Amt','Mean_DOW_Order_Amt']]\n",
    "    analysis_df['Constant'] = 1\n",
    "    X = analysis_df['Constant'] \n",
    "#     X = sm.add_constant(X)\n",
    "    Y = analysis_df[['Order_Amt']]\n",
    "    residuals_df = sm.OLS(Y.astype(float), X.astype(float)).fit()\n",
    "    \n",
    "    X2 = analysis_df[['Customer_ID']]\n",
    "    X2['Residual'] = residuals_df.resid\n",
    "    X2['Constant'] = 1\n",
    "    clustered_res = sm.OLS(X2['Residual'], X2['Constant']).fit(method='pinv'). \\\n",
    "                       get_robustcov_results('cluster', groups = X2['Customer_ID'], \n",
    "                       use_correction=True, df_correction=True)\n",
    "    \n",
    "    clustered_sd = clustered_res.bse[0] * np.sqrt(analysis_df.shape[0])\n",
    "    effect_size = absolute_mde / clustered_sd\n",
    "    recommended_n = int(sm.stats.tt_ind_solve_power(effect_size = effect_size, \n",
    "                        alpha = rejection_region, power = desired_power, \n",
    "                        alternative = 'larger'))\n",
    "    print(\"A sample size of {} was recommended.\".format(recommended_n ))\n",
    "    return recommended_n, absolute_mde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_sample_size_est(sample_size, data_src, data_labels, alpha, verify_n_times):\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "    import pandas as pd\n",
    "    \n",
    "    SAMPLE_SIZE = sample_size\n",
    "    VERIFICATION_ITERATIONS = verify_n_times\n",
    "    ALPHA = alpha\n",
    "\n",
    "    i = 0\n",
    "    pvals  = []\n",
    "    r_sqr  = []\n",
    "    cond_n = []\n",
    "    while i < VERIFICATION_ITERATIONS:\n",
    "        working_df = data_src.sample(SAMPLE_SIZE, replace=True)\n",
    "        ###############################################################################\n",
    "        pa_retailer_means = pd.DataFrame(working_df.groupby(['Retailer_ID'])['Order_Amt'].mean())\n",
    "        pa_retailer_means.reset_index(inplace=True)\n",
    "        pa_retailer_means.columns = ['Retailer_ID', 'Mean_Retailer_Order_Amt']\n",
    "        ###############################################################################\n",
    "        pa_dow_means = pd.DataFrame(working_df.groupby(['Dow_Rand'])['Order_Amt'].mean())\n",
    "        pa_dow_means.reset_index(inplace=True)\n",
    "        pa_dow_means.columns = ['Dow_Rand', 'Mean_DOW_Order_Amt']\n",
    "        ###############################################################################\n",
    "        analysis_df = pd.merge(working_df, pa_retailer_means, on='Retailer_ID', how='left')\n",
    "        analysis_df = pd.merge(analysis_df, pa_dow_means, on='Dow_Rand', how='left')\n",
    "        ###############################################################################\n",
    "        analysis_df = analysis_df[['Order_Amt', 'Customer_ID', 'Treated', 'Mean_Order_Amt', \n",
    "                                 'Mean_Retailer_Order_Amt','Mean_DOW_Order_Amt']]\n",
    "        ###############################################################################\n",
    "        analysis_df = analysis_df.dropna(how = 'any')\n",
    "        \n",
    "#         X = analysis_df[['Treated', 'Mean_Order_Amt', 'Mean_Retailer_Order_Amt','Mean_DOW_Order_Amt']]\n",
    "#         X = analysis_df[['Treated','Mean_DOW_Order_Amt']]\n",
    "        X = analysis_df[['Treated']]\n",
    "        X = sm.add_constant(X)\n",
    "        Y = analysis_df[['Order_Amt']]\n",
    "        model = sm.OLS(Y.astype(float), X.astype(float)).fit(method='pinv'). \\\n",
    "                       get_robustcov_results('cluster', groups = analysis_df['Customer_ID'], \n",
    "                       use_correction=True, df_correction=True)\n",
    "        try:\n",
    "            if model.pvalues[1] < ALPHA: \n",
    "                pvals.append(1)\n",
    "            else:\n",
    "                pvals.append(0)  \n",
    "        except:\n",
    "            print(model.summary())\n",
    "        r_sqr.append(model.rsquared_adj)\n",
    "        cond_n.append(model.condition_number)\n",
    "        i += 1\n",
    "        if i % int((VERIFICATION_ITERATIONS)/10.0) == 0:\n",
    "            completion = str(round((i/VERIFICATION_ITERATIONS)*100, 2))+'%'\n",
    "            print(completion + ' complete.')\n",
    "            \n",
    "    # ----- Exit inner loop     \n",
    "    x = ['Treated']\n",
    "    str_out = 'Order_Amt =' \n",
    "    d = 0\n",
    "    for i in x:\n",
    "        if d < 1:\n",
    "            k = \" '\" + i + \"'\"\n",
    "        else:\n",
    "            k = \" + '\" + i + \"'\"\n",
    "        str_out += k\n",
    "        d += 1    \n",
    "    \n",
    "    actual_power = sum(pvals)/len(pvals)  \n",
    "    mean_r_sqr   = sum(r_sqr)/len(r_sqr)   \n",
    "    mean_cond_n  = sum(cond_n)/len(cond_n)  \n",
    "    print(\"Actual power was estimated at {}.\".format(actual_power))\n",
    "    return actual_power, mean_r_sqr, mean_cond_n, str_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_rec(pa_file_in, analysis_csv_in, headers_in, alpha, desired_power, point_verifications):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy.optimize import curve_fit\n",
    "    \n",
    "    a_src = pd.read_csv(pa_file_in)\n",
    "    b_src = pd.read_csv(analysis_csv_in)\n",
    "\n",
    "    recommended_n, absolute_mde = create_sample_size_rec(a_src, headers_in, alpha, desired_power)\n",
    "\n",
    "    if int(recommended_n*0.0005) < 10:\n",
    "        pt_1 = 10    \n",
    "    else:\n",
    "        pt_1 = int(recommended_n*0.001)\n",
    "    if int(recommended_n*0.005) < 10:\n",
    "        pt_2 = 10    \n",
    "    else:\n",
    "        pt_2 = int(recommended_n*0.01)\n",
    "    if int(recommended_n*0.05) < 10:\n",
    "        pt_3 = 10    \n",
    "    else:\n",
    "        pt_3 = int(recommended_n*0.1)\n",
    "    pt_4 = int(recommended_n)\n",
    "    print(\"Proposed the following points [{},{},{},{}] for pass 1.\".format(pt_1,pt_2,pt_3,pt_4))\n",
    "\n",
    "    points = [pt_1, pt_2, pt_3, pt_4]\n",
    "\n",
    "    actual_power_pass_1 = []\n",
    "    for i in points:\n",
    "        actual_power, mean_r_sqr, mean_cond_n, str_out = verify_sample_size_est(i, \n",
    "                                                                                b_src, \n",
    "                                                                                headers_in, \n",
    "                                                                                alpha, \n",
    "                                                                                point_verifications)\n",
    "        actual_power_pass_1.append(actual_power)\n",
    "        \n",
    "    df = pd.DataFrame(points, actual_power_pass_1)\n",
    "    df = df.reset_index(drop=False)\n",
    "    df.columns = ['power', 'n']\n",
    "    ub = df[df['power'] > 0.8]\n",
    "    if len(ub) > 0:\n",
    "        ub.sort_values('n', inplace=True, ascending=True)\n",
    "        ub = ub.iat[0,1]\n",
    "    else:\n",
    "        df.sort_values('n', inplace=True, ascending=False)\n",
    "        ub = int(df.iat[0,1]*10)\n",
    "    lb = df[df['power'] < 0.8]\n",
    "    lb.sort_values('n', inplace=True, ascending=False)\n",
    "    lb = lb.iat[0,1]\n",
    "    print('The upper-bound for pass 1 was found to be {}.'.format(ub))\n",
    "    print('The lower-bound for pass 1 was found to be {}.'.format(lb))\n",
    "    new_points = []\n",
    "    for i in np.linspace(lb, ub, 5, endpoint = False):\n",
    "        new_points.append(int(i))\n",
    "    \n",
    "    print(\"Proposed the following points [{},{},{},{},{}] for pass 2.\".format(new_points[0],\n",
    "                                                                              new_points[1],\n",
    "                                                                              new_points[2],\n",
    "                                                                              new_points[3],\n",
    "                                                                              new_points[4]))\n",
    "    actual_power_pass_2 = []\n",
    "    for i in new_points:\n",
    "        actual_power, mean_r_sqr, mean_cond_n, str_out = verify_sample_size_est(i, \n",
    "                                                                                b_src, \n",
    "                                                                                headers_in, \n",
    "                                                                                alpha, \n",
    "                                                                                point_verifications)\n",
    "        actual_power_pass_2.append(actual_power)\n",
    "        \n",
    "    df = pd.DataFrame(new_points, actual_power_pass_2)\n",
    "    df = df.reset_index(drop=False)\n",
    "    df.columns = ['power', 'n']\n",
    "    print(df)\n",
    "\n",
    "    def exp_func(x, a, b, c):\n",
    "        return a * np.log(b * x) + c\n",
    "    desired_power = 0.8\n",
    "    cdf = df['power']\n",
    "    eta = df['n']\n",
    "    popt, pcov = curve_fit(exp_func, eta, cdf)\n",
    "    recommended_n = int(np.exp((desired_power - popt[2])/popt[0])/popt[1])\n",
    "    return recommended_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_analysis(meta_n, power_n, final_verification_n, file_a, file_b, file_out):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "\n",
    "    dir = '/home/bknight/Documents/Power_Analysis_Techniques/variance_of_residuals/residual_dfs/'\n",
    "    a_src    = dir + file_a\n",
    "    b_src    = dir + file_b\n",
    "    b_src_df = pd.read_csv(b_src)\n",
    "    headers_in = ['Order_ID', 'Customer_ID', 'Mean_Order_Amt', 'Treated',\n",
    "                  'Treatment_Modifier', 'Retailer_ID', 'Retailer_Scalar',\n",
    "                  'Dow_Rand', 'DOW', 'Noise', 'Order_Amt']\n",
    "    \n",
    "    power_ls = []\n",
    "    n_ls = []\n",
    "    pass_1_p_ls = []\n",
    "    pass_1_n_ls = []\n",
    "    pass_2_p_ls = []\n",
    "    pass_2_n_ls = []\n",
    "    mean_r_sqr_ls = []\n",
    "    mean_cond_n_ls = []\n",
    "    str_out_ls = []\n",
    "    verification_n_ls = [] \n",
    "    estimation_time_ls = []\n",
    "    for i in list(range(0,meta_n,1)):\n",
    "        start = time.time()\n",
    "        n = create_n_rec(a_src, b_src, headers_in, 0.05, 0.8, power_n)\n",
    "        print(\"A final value of {} was recommended.\".format(n))\n",
    "        end = time.time()\n",
    "        delta = (end - start)/60.0\n",
    "        print(\"The estimation took {} minutes.\".format(delta))\n",
    "        actual_power, mean_r_sqr, mean_cond_n, str_out = verify_sample_size_est(n, b_src_df, \n",
    "                                                         headers_in, 0.05, final_verification_n)\n",
    "        print(\"The actual power attained was {}.\".format(actual_power))\n",
    "        \n",
    "        power_ls.append(actual_power)\n",
    "        n_ls.append(n)\n",
    "        mean_r_sqr_ls.append(mean_r_sqr)\n",
    "        mean_cond_n_ls.append(mean_cond_n)\n",
    "        str_out_ls.append(str_out)\n",
    "        pass_1_p_ls.append(4)\n",
    "        pass_1_n_ls.append(power_n)\n",
    "        pass_2_p_ls.append(5)\n",
    "        pass_2_n_ls.append(power_n)\n",
    "        verification_n_ls.append(final_verification_n)\n",
    "        estimation_time_ls.append(delta)\n",
    "\n",
    "    df_out = pd.DataFrame(\n",
    "        {'Effectve Power': power_ls,\n",
    "         'Recommended N': n_ls,\n",
    "         'Mean R-Squared': mean_r_sqr_ls,\n",
    "         'Mean Cond. No.': mean_cond_n_ls,\n",
    "         'Specification': str_out_ls,\n",
    "         'Initial Pass Points': pass_1_p_ls, \n",
    "         'Initial Pass Iterations per Point': pass_1_n_ls,\n",
    "         'Second Pass Points': pass_2_p_ls,\n",
    "         'Second Pass Iterations per Point': pass_2_n_ls,\n",
    "         'Verification Iterations': verification_n_ls,\n",
    "         'Estimation Time': estimation_time_ls\n",
    "        })\n",
    "    df_out.to_csv(file_out)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The absolute MDE was estimated as 0.42411496603367027.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bknight/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/bknight/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/bknight/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample size of 492075 was recommended.\n",
      "Proposed the following points [492,4920,49207,492075] for pass 1.\n",
      "10.0% complete.\n",
      "20.0% complete.\n",
      "30.0% complete.\n",
      "40.0% complete.\n",
      "50.0% complete.\n",
      "60.0% complete.\n",
      "70.0% complete.\n",
      "80.0% complete.\n",
      "90.0% complete.\n",
      "100.0% complete.\n",
      "Actual power was estimated at 0.05.\n",
      "10.0% complete.\n",
      "20.0% complete.\n",
      "30.0% complete.\n",
      "40.0% complete.\n",
      "50.0% complete.\n",
      "60.0% complete.\n",
      "70.0% complete.\n",
      "80.0% complete.\n",
      "90.0% complete.\n",
      "100.0% complete.\n",
      "Actual power was estimated at 0.09.\n",
      "10.0% complete.\n",
      "20.0% complete.\n",
      "30.0% complete.\n",
      "40.0% complete.\n",
      "50.0% complete.\n",
      "60.0% complete.\n",
      "70.0% complete.\n",
      "80.0% complete.\n",
      "90.0% complete.\n",
      "100.0% complete.\n",
      "Actual power was estimated at 0.18.\n",
      "10.0% complete.\n",
      "20.0% complete.\n",
      "30.0% complete.\n",
      "40.0% complete.\n",
      "50.0% complete.\n",
      "60.0% complete.\n",
      "70.0% complete.\n",
      "80.0% complete.\n",
      "90.0% complete.\n",
      "100.0% complete.\n",
      "Actual power was estimated at 0.725.\n",
      "The upper-bound for pass 1 was found to be 4920750.\n",
      "The lower-bound for pass 1 was found to be 492075.\n",
      "Proposed the following points [492075,1377810,2263545,3149280,4035015] for pass 2.\n",
      "10.0% complete.\n",
      "20.0% complete.\n",
      "30.0% complete.\n",
      "40.0% complete.\n",
      "50.0% complete.\n",
      "60.0% complete.\n",
      "70.0% complete.\n",
      "80.0% complete.\n",
      "90.0% complete.\n",
      "100.0% complete.\n",
      "Actual power was estimated at 0.63.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d3a71bae99ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m df = meta_analysis(100, 200, 500, 'part_II_df_mde_0_005_n_100000_a.csv', \n\u001b[1;32m      2\u001b[0m                                   \u001b[0;34m'part_II_df_mde_0_005_n_100000_b.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                   'comb_methd_model_IV_mde_0_005_n_100000.csv')\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-adfe3f6bde35>\u001b[0m in \u001b[0;36mmeta_analysis\u001b[0;34m(meta_n, power_n, final_verification_n, file_a, file_b, file_out)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeta_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_n_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A final value of {} was recommended.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ecad631446b3>\u001b[0m in \u001b[0;36mcreate_n_rec\u001b[0;34m(pa_file_in, analysis_csv_in, headers_in, alpha, desired_power, point_verifications)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                                                                 \u001b[0mheaders_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                                                 \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                                                                 point_verifications)\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mactual_power_pass_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c2c05a0cdce2>\u001b[0m in \u001b[0;36mverify_sample_size_est\u001b[0;34m(sample_size, data_src, data_labels, alpha, verify_n_times)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcond_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mVERIFICATION_ITERATIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mworking_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAMPLE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpa_retailer_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Retailer_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Order_Amt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   4199\u001b[0m                              \"provide positive value.\")\n\u001b[1;32m   4200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4201\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "df = meta_analysis(100, 200, 500, 'part_II_df_mde_0_005_n_100000_a.csv', \n",
    "                                  'part_II_df_mde_0_005_n_100000_b.csv',\n",
    "                                  'comb_methd_model_IV_mde_0_005_n_100000.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
