{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_power(sample_sizes_to_eval, sub_iterations, meta_iterations,\n",
    "                p_value_threshold, desired_power, data, file_out, seed): \n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "    import numpy as np\n",
    "    import time    \n",
    "    from scipy.optimize import curve_fit\n",
    "    start = time.time()\n",
    "    \n",
    "    meta_results        = []\n",
    "    meta_supplement     = []\n",
    "    sub_iterations_list = []\n",
    "    \n",
    "    max_n = len(data)\n",
    "    \n",
    "    master_counter = 0\n",
    "    while master_counter < meta_iterations:\n",
    "        try:\n",
    "            master_results = []\n",
    "            for k in sample_sizes_to_eval:\n",
    "                counter = 0 \n",
    "                pvalues = []\n",
    "                while counter < sub_iterations:\n",
    "                    working_df = data.sample(k, replace=False, random_state=seed)\n",
    "                    X = working_df.Treated\n",
    "                    X = sm.add_constant(X)\n",
    "                    Y = working_df.Order_Amt\n",
    "                    model = sm.OLS(Y, X).fit(method='pinv').get_robustcov_results(\n",
    "                            'cluster', groups = working_df.Customer_ID, \n",
    "                            use_correction=True, df_correction=True)\n",
    "                    pvalues.append(model.pvalues[1])\n",
    "                    counter += 1\n",
    "\n",
    "                results = []\n",
    "                for i in pvalues:\n",
    "                    if i <= p_value_threshold:\n",
    "                        results.append(1)    \n",
    "                    else:\n",
    "                        results.append(0)  \n",
    "                out = []\n",
    "                out.append(k)\n",
    "                out.append(p_value_threshold)\n",
    "                p_val = sum(results)/len(results)\n",
    "                print(p_val)\n",
    "                out.append(p_val) \n",
    "                master_results.append(out)\n",
    "\n",
    "            def exp_func(x, a, b, c):\n",
    "                return a * np.log(b * x) + c\n",
    "\n",
    "            eta = []\n",
    "            for i in master_results:\n",
    "                eta.append(i[0])\n",
    "            eta = np.asarray(eta)\n",
    "\n",
    "            cdf = []\n",
    "            for i in master_results:\n",
    "                cdf.append(i[2])\n",
    "            cdf = np.asarray(cdf)\n",
    "\n",
    "            popt, pcov = curve_fit(exp_func, eta, cdf)\n",
    "\n",
    "\n",
    "            recommended_n = int(np.exp((desired_power - popt[2])/popt[0])/popt[1])\n",
    "            if recommended_n > max_n:\n",
    "                recommended_n = max_n\n",
    "\n",
    "            print(\"The recommended sample size for a statistical power level of \" +\n",
    "                  \"{} is {}.\".format(desired_power, recommended_n))\n",
    "\n",
    "            final_pvalues = []\n",
    "            counter_2 = 0 \n",
    "            while counter_2 < 1000:\n",
    "                if counter_2 % 100 == 0:                       \n",
    "                    print(\"Verification {} complete.\".format(round((counter_2/1000), 2)))\n",
    "                working_df_2 = data.sample(recommended_n, replace=False, random_state=seed)\n",
    "                X = working_df_2.Treated\n",
    "                X = sm.add_constant(X)\n",
    "                Y = working_df_2.Order_Amt\n",
    "                model_2 = sm.OLS(Y, X).fit(method='pinv').get_robustcov_results(\n",
    "                          'cluster', groups = working_df_2.Customer_ID, \n",
    "                          use_correction=True, df_correction=True)\n",
    "                final_pvalues.append(model_2.pvalues[1])\n",
    "                counter_2 += 1\n",
    "\n",
    "            final_results = []\n",
    "            for i in final_pvalues:\n",
    "                if i <= p_value_threshold:\n",
    "                    final_results.append(1)    \n",
    "                else:\n",
    "                    final_results.append(0)  \n",
    "\n",
    "            meta_results.append(sum(final_results)/len(final_results))\n",
    "            meta_supplement.append(desired_power)\n",
    "            sub_iterations_list.append(sub_iterations)\n",
    "            master_counter += 1\n",
    "            print(\"{} iterations sucessfully completed.\".format(master_counter))\n",
    "            end = time.time()\n",
    "            print(str(end - start) + ' time elapsed.')\n",
    "        except:\n",
    "            print(\"Power curve inference failed.\")        \n",
    "        \n",
    "    df_out = pd.DataFrame(list(zip(meta_supplement, meta_results))) \n",
    "    df_out.columns = ['Desired Power', 'Actual Power']\n",
    "    df_out['Error'] = (df_out['Desired Power'] - df_out['Actual Power'])\n",
    "    df_out['Absolute Error'] =  df_out['Error'].abs()\n",
    "    df_out['Iterations'] = sub_iterations_list\n",
    "    total_iterations = (len(sample_sizes_to_eval)*sub_iterations)+1000\n",
    "    df_out['Total Iterations'] = total_iterations\n",
    "    df_out.to_csv(file_out)\n",
    "#     return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_sim(sample_size_vector,\n",
    "              simulations, meta_simulations,  \n",
    "              alpha, desired_power, \n",
    "              dataframe, file_out):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import multiprocessing \n",
    "    \n",
    "    meta_n = int(meta_simulations/5)\n",
    "    \n",
    "    k = int(len(dataframe)/5.0)\n",
    "    \n",
    "    print(\"Will perform {} set(s) of {} simulations each partitioned 5 ways, or {} per process\".\n",
    "          format(meta_simulations, simulations, meta_n))\n",
    "\n",
    "    p1 = multiprocessing.Process(target=infer_power, \n",
    "             args=(sample_size_vector, simulations, meta_n, alpha, \n",
    "                   desired_power, dataframe.sample(k, replace=False), \n",
    "                   'multi_sim_df1.csv', 1)) \n",
    "    p2 = multiprocessing.Process(target=infer_power, \n",
    "             args=(sample_size_vector, simulations, meta_n, alpha, \n",
    "                   desired_power, dataframe.sample(k, replace=False), \n",
    "                   'multi_sim_df2.csv', 2)) \n",
    "    p3 = multiprocessing.Process(target=infer_power, \n",
    "             args=(sample_size_vector, simulations, meta_n, alpha, \n",
    "                   desired_power, dataframe.sample(k, replace=False), \n",
    "                   'multi_sim_df3.csv', 3)) \n",
    "    p4 = multiprocessing.Process(target=infer_power, \n",
    "             args=(sample_size_vector, simulations, meta_n, alpha, \n",
    "                   desired_power, dataframe.sample(k, replace=False), \n",
    "                   'multi_sim_df4.csv', 4)) \n",
    "    p5 = multiprocessing.Process(target=infer_power, \n",
    "             args=(sample_size_vector, simulations, meta_n, alpha, \n",
    "                   desired_power, dataframe.sample(k, replace=False), \n",
    "                   'multi_sim_df5.csv', 5)) \n",
    "\n",
    "    p1.start() \n",
    "    p2.start()\n",
    "    p3.start() \n",
    "    p4.start() \n",
    "    p5.start() \n",
    "\n",
    "    p1.join() \n",
    "    p2.join() \n",
    "    p3.join() \n",
    "    p4.join() \n",
    "    p5.join() \n",
    "\n",
    "    df1 = pd.read_csv('multi_sim_df1.csv')\n",
    "    df2 = pd.read_csv('multi_sim_df2.csv')\n",
    "    df3 = pd.read_csv('multi_sim_df3.csv')\n",
    "    df4 = pd.read_csv('multi_sim_df4.csv')\n",
    "    df5 = pd.read_csv('multi_sim_df5.csv')\n",
    "    df  = pd.concat([df1,df2], axis=0)\n",
    "    df  = pd.concat([df ,df3], axis=0)\n",
    "    df  = pd.concat([df ,df4], axis=0)\n",
    "    df  = pd.concat([df ,df5], axis=0)\n",
    "\n",
    "    try:\n",
    "        os.remove('multi_sim_df1.csv')\n",
    "    except:\n",
    "        print('The file df1.csv could not be deleted.')\n",
    "    try:\n",
    "        os.remove('multi_sim_df2.csv')\n",
    "    except:\n",
    "        print('The file df2.csv could not be deleted.')\n",
    "    try:\n",
    "        os.remove('multi_sim_df3.csv')\n",
    "    except:\n",
    "        print('The file df3.csv could not be deleted.')\n",
    "    try:\n",
    "        os.remove('multi_sim_df4.csv')\n",
    "    except:\n",
    "        print('The file df4.csv could not be deleted.')\n",
    "    try:\n",
    "        os.remove('multi_sim_df5.csv')\n",
    "    except:\n",
    "        print('The file df5.csv could not be deleted.')\n",
    "        \n",
    "#     return df\n",
    "    df.to_csv(file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_file_to_read = pd.read_csv('data_c_1000000_mde_01.csv')\n",
    "multi_sim([8000,10000,12000], 100, 5, 0.05, 0.8, data_file_to_read, 'results_8k_10k_12k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data_file_to_read = pd.read_csv('data_c_1000000_mde_01.csv')\n",
    "# infer_power([          8000,10000,12000],             20, 1, 0.05, 0.8, data_file_to_read, 'df1.csv')\n",
    "# print(\"DF 1 created.\")\n",
    "# infer_power([     6000,8000,10000,12000,14000],       20, 1, 0.05, 0.8, data_file_to_read, 'df2.csv')\n",
    "# print(\"DF 2 created.\")\n",
    "# infer_power([4000,6000,8000,10000,12000,14000,16000], 20, 1, 0.05, 0.8, data_file_to_read, 'df3.csv')\n",
    "# print(\"DF 3 created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(1, figsize=(12, 6))\n",
    "\n",
    "# # Create an axes instance\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# # Create the boxplot\n",
    "# bp = ax.boxplot(data)\n",
    "# plt.ylim(0.0,0.2)\n",
    "\n",
    "# ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "# ax.tick_params(axis='x', which='major', labelsize=10)\n",
    "# ax.set_xticklabels([\"Sample I\\n500 Iterations\", \"Sample II\\n500 Iterations\", \"Sample III\\n500 Iterations\",\n",
    "#                     \"Sample IV\\n500 Iterations\", \"Sample V\\n500 Iterations\", \"Sample VI\\n500 Iterations\",\n",
    "#                     \"Sample VII\\n500 Iterations\", \"Sample VIII\\n500 Iterations\", \"Sample IX\\n500 Iterations\"])\n",
    "\n",
    "# style = dict(size=14, color='blue')\n",
    "# # ax.text(1, -0.1, \"\\u03bc +/-\", ha='center', **style)\n",
    "# # fig.savefig('fig1.png', bbox_inches='tight')\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_data_frame(list_of_sample_sizes, sub_iterations, \n",
    "#                       meta_iterations, rejection_region, target_power,\n",
    "#                       csv_in):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import numpy as np\n",
    "#     import scipy.stats as stats\n",
    "#     import math\n",
    "    \n",
    "# df = infer_power(list_of_sample_sizes, sub_iterations, \n",
    "#                  meta_iterations, rejection_region, target_power, csv_in)\n",
    "# mu               = df['Error'].mean()\n",
    "# sigma            = df['Error'].std()\n",
    "# iterations       = df['Iterations'].unique()[0]\n",
    "# total_iterations = df['Total Iterations'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def x(dataframe, verification_sim_n)\n",
    "\n",
    "#     k = int(len(dataframe)/5.0)    \n",
    "#     while counter_2 < verification_sim_n:\n",
    "#         sample_1 = data.sample(k, replace=False)\n",
    "#         sample_2 = data.sample(k, replace=False)\n",
    "#         sample_3 = data.sample(k, replace=False)\n",
    "#         sample_4 = data.sample(k, replace=False)\n",
    "#         sample_5 = data.sample(k, replace=False)\n",
    "\n",
    "    \n",
    "\n",
    "#         working_df = data.sample(recommended_n, replace=False)\n",
    "#         X = working_df.Treated\n",
    "#         X = sm.add_constant(X)\n",
    "#         Y = working_df.Order_Amt\n",
    "#         model = sm.OLS(Y, X).fit(method='pinv').get_robustcov_results(\n",
    "#                   'cluster', groups = working_df.Customer_ID, \n",
    "#                   use_correction=True, df_correction=True)\n",
    "#         final_pvalues.append(model.pvalues[1])\n",
    "#         counter_2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # results_sd_3_n_500 = infer_power([8000, 10000, 12000], \n",
    "# #                                   500, 2, 0.05, 0.8, 'data_c_1000000_mde_01.csv')\n",
    "\n",
    "\n",
    "\n",
    "# x = np.linspace(mu - 3*sigma, mu + 3*sigma, 1000)\n",
    "# plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "# plt.axvline(x=0, color = 'red', alpha=0.4)\n",
    "# plt.xlim(-0.1,0.5)\n",
    "# plt.ylim(0.0,15)\n",
    "# bbox = dict(boxstyle=\"round\", fc=\"0.9\")\n",
    "# plt.annotate('Mean Abs. Error  = {} \\nError Std. = {} \\nIterations per Point = {} \\nTotal Iterations = {}'.\n",
    "#              format(round(mu,2), round(sigma,2), iterations, total_iterations),\n",
    "#             (5, 0), xytext=(195, 177),\n",
    "#             xycoords='figure pixels',\n",
    "#             textcoords='offset points',\n",
    "#             size = 12,\n",
    "#             bbox=bbox)\n",
    "# plt.show()\n",
    "# # plt.savefig('6k_10k_14k_n500.png')\n",
    "# plt.clf()\n",
    "\n",
    "# fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# # Create an axes instance\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# # Create the boxplot\n",
    "# data = [results_sd_3_n_500['Error'], [0.5, 0.6, 0.2, 0.1]]\n",
    "# bp = ax.boxplot(data)\n",
    "# plt.ylim(-1.0,1.0)\n",
    "# ax.set_xticklabels(['Sample1', 'Sample2', 'Sample3', 'Sample4'])\n",
    "# # fig.savefig('fig1.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [df1['Error'], df2['Error'], df3['Error'], df4['Error'], df5['Error'], df6['Error'],\n",
    "        df7['Error'], df8['Error'], df9['Error']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
